# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wT30wE8qX-561lUYmCxGTr8OqJI-PqgK
"""

import re
import pickle
import random

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from string import punctuation
from keras.preprocessing.text import Tokenizer
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
#import math
import itertools

file = open("/content/brown.txt","r")
data = file.read()
data = data.lower()
#data = remove_stopwords(data)
#print(data)
all_text = ''.join([c for c in data if c not in punctuation])
#print(len(all_text))
text_split = all_text.split('\n')

def create_seq(text, seq_len = 1):
    
    sequences = []

    # if the number of tokens in 'text' is greater than 5
    if len(text.split()) > seq_len:
      for i in range(seq_len, len(text.split())):
        # select sequence of tokens
        seq = text.split()[i-seq_len:i+1]
        # add to the list
        sequences.append(" ".join(seq))
        

      return sequences

    # if the number of tokens in 'text' is less than or equal to 5
    else:
      
      return [text]

words_list = [create_seq(i) for i in text_split ]

# merge list-of-lists into a single list
#words_list = sum(words_list, [])
count=len(words_list)
# count of sequences
random.shuffle(words_list)
train = words_list[:math.ceil(count*0.7)]
validation = words_list[math.ceil(count*0.7):math.ceil(count*0.7)+math.ceil(count*0.2)]
test = words_list[math.ceil(count*0.7)+math.ceil(count*0.2):]
print(train[0])
#print(seqs[0])
#print(seqs)

train_data = list(itertools.chain.from_iterable(train))
x_train = []
y_train = []

for s in train_data:
  x_train.append(" ".join(s.split()[:-1]))
  y_train.append(" ".join(s.split()[1:]))

validation_data = list(itertools.chain.from_iterable(validation))
x_val = []
y_val = []

for s in validation_data:
  x_val.append(" ".join(s.split()[:-1]))
  y_val.append(" ".join(s.split()[1:]))

#print(x)
print(x_train[:100])

int2token = {}
cnt = 0

for w in set(" ".join(text_split).split()):
  int2token[cnt] = w
  cnt+= 1

# create token-to-integer mapping
token2int = {t: i for i, t in int2token.items()}

vocab_size = len(int2token)
vocab_size

def get_integer_seq(seq):
  return [token2int[w] for w in seq.split()]
#x_train_int=np.array([])
# convert text sequences to integer sequences
x_int = [get_integer_seq(i) for i in x_train]
x_int = list(filter(None, x_int))
y_int = [get_integer_seq(i) for i in y_train]
y_int = list(filter(None, y_int))
print(x_int[:50])
# convert lists to numpy arrays
x_train_int = np.asarray(x_int)
y_train_int = np.asarray(y_int)

print(y_train_int)

x_int = [get_integer_seq(i) for i in x_val]
y_int = [get_integer_seq(i) for i in y_val]
x_int = list(filter(None, x_int))
y_int = list(filter(None, y_int))
# convert lists to numpy arrays
x_val_int = np.array(x_int)
y_val_int = np.array(y_int)
print(x_val_int)

def get_batches(arr_x, arr_y, batch_size):
         
    prv = 0
    for n in range(batch_size, arr_x.shape[0], batch_size):
      x = arr_x[prv:n]
      y = arr_y[prv:n]
      prv = n
      yield x, y

class WordLSTM(nn.Module):
    
    def __init__(self, n_hidden=256, n_layers=1, drop_prob=0.3, lr=0.001):
        super().__init__()

        self.drop_prob = drop_prob
        self.n_layers = n_layers
        self.n_hidden = n_hidden
        self.lr = lr
        
        self.emb_layer = nn.Embedding(vocab_size, 200)
        self.lstm = nn.LSTM(200, n_hidden, n_layers, dropout=drop_prob, batch_first=True)
        self.dropout = nn.Dropout(drop_prob)
        self.fc = nn.Linear(n_hidden, vocab_size)      
    
    def forward(self, x, hidden):
        embedded = self.emb_layer(x)     
        lstm_output, hidden = self.lstm(embedded, hidden)
        out = self.dropout(lstm_output)
        out = out.reshape(-1, self.n_hidden) 

        out = self.fc(out)

        # return the final output and the hidden state
        return out, hidden
    
    
    def init_hidden(self, batch_size):
        ''' initializes hidden state '''
        # Create two new tensors with sizes n_layers x batch_size x n_hidden,
        # initialized to zero, for hidden state and cell state of LSTM
        weight = next(self.parameters()).data

        # if GPU is available
        if (torch.cuda.is_available()):
          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),
                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())
        
        # if GPU is not available
        else:
          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),
                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())
        
        return hidden

net = WordLSTM()
print(net)

def training(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):
    
    # optimizer
    opt = torch.optim.Adam(net.parameters(), lr=lr)
    
    # loss
    criterion = nn.CrossEntropyLoss()
    
    # push model to GPU
    net.cuda()
    
    counter = 0

    net.train()

    for e in range(epochs):

        # initialize hidden state
        h = net.init_hidden(batch_size)
        
        for x, y in get_batches(x_train_int, y_train_int, batch_size):
            counter+= 1
            
            # convert numpy arrays to PyTorch arrays
            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)
            
            # push tensors to GPU
            inputs, targets = inputs.cuda(), targets.cuda()

            # detach hidden states
            h = tuple([each.data for each in h])

            # zero accumulated gradients
            net.zero_grad()
            
            # get the output from the model
            output, h = net(inputs, h)
            
            # calculate the loss and perform backprop
            loss = criterion(output, targets.view(-1))

            # back-propagate error
            loss.backward()

            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.
            nn.utils.clip_grad_norm_(net.parameters(), clip)

            # update weigths
            opt.step()            
            
            if counter % print_every == 0:
            
              print("Epoch: {}/{}...".format(e+1, epochs),
                    "Step: {}...".format(counter),"loss-->"+str(loss))

training(net, batch_size = 256, epochs=5, print_every=256)

def predict(net, tkn, h=None):
         
  # tensor inputs
  x = np.array([[token2int[tkn]]])
  inputs = torch.from_numpy(x)
  
  # push to GPU
  inputs = inputs.cuda()

  # detach hidden state from history
  h = tuple([each.data for each in h])

  out, h = net(inputs, h)
  
  p = F.softmax(out, dim=1).data

  p = p.cpu()

  p = p.numpy()
  p = p.reshape(p.shape[1],)

  return np.amax(p)


def sample(net, size, prime='it is'):

    net.cuda()
    
    net.eval()

    h = net.init_hidden(1)

    toks = prime.split()

    for t in prime.split():
      prob = predict(net, t, h)
    
    return prob

tr=[]
for li in train:
  arr=[]
  for w in li:
    arr.append(w.split())
  flat_list = list(itertools.chain(*arr))
  res = [i for n, i in enumerate(flat_list) if i not in flat_list[:n]]
  tr.append(res)
print(tr[:5])

final=[]
tr = list(filter(None, tr))
for lst in tr:
  result=[]
  for wrd in lst:
    result.append(sample(net, 1, prime = wrd))
  final.append(result)
#print(final)
print(final[:5])

perplexity_train=[]
sum=0
cnt=0
c=0
for l in final:
  probability = np.prod(l)
  if probability == 0:
    probability = 0.075
  res = probability ** (-1/len(l))
  perplexity_train.append(res)
  cnt+=1
print(perplexity_train)
avg_train = math.fsum(perplexity_train)/cnt
print("average perplexity of train dataset: "+str(avg_train))

f1 = open("/content/train_file_neural model.txt","w")
j = 0
for ls in tr:
  sentence = " ".join(ls)
  f1.write(sentence + "\t" + str(perplexity_train[j]) + "\n")
  j += 1
f1.write("\n")
f1.write("average perplexity of test dataset: "+str(avg_train))

val=[]
for li in validation:
  arr=[]
  for w in li:
    arr.append(w.split())
  flat_list = list(itertools.chain(*arr))
  res = [i for n, i in enumerate(flat_list) if i not in flat_list[:n]]
  val.append(res)
print(val[:5])

final=[]
val = list(filter(None, val))
for lst in val:
  result=[]
  for wrd in lst:
    result.append(sample(net, 1, prime = wrd))
  final.append(result)
#print(final)
print(final[:5])

perplexity=[]
sum=0
cnt=0
c=0
for l in final:
  probability = np.prod(l)
  if probability == 0:
    probability = 0.075
  res = probability ** (-1/len(l))
  perplexity.append(res)
  cnt+=1
print(perplexity)
avg = math.fsum(perplexity)/cnt
print("average perplexity of validation dataset: "+str(avg))

te=[]
for li in test:
  arr=[]
  for w in li:
    arr.append(w.split())
  flat_list = list(itertools.chain(*arr))
  res = [i for n, i in enumerate(flat_list) if i not in flat_list[:n]]
  te.append(res)
print(te[:5])

final=[]
te = list(filter(None, te))
for lst in te:
  result=[]
  for wrd in lst:
    result.append(sample(net, 1, prime = wrd))
  final.append(result)
#print(final)
print(final[:5])

perplexity_test=[]
sum=0
cnt=0
c=0
for l in final:
  probability = np.prod(l)
  if probability == 0:
    probability = 0.075
  res = probability ** (-1/len(l))
  perplexity_test.append(res)
  cnt+=1
print(perplexity_test)
avg_test = math.fsum(perplexity_test)/cnt
print("average perplexity of test dataset: "+str(avg_test))

f3 = open("/content/test_file_neural model.txt","w")
k = 0
for ls in te:
  sentence = " ".join(ls)
  f3.write(sentence + "\t" + str(perplexity_test[k]) + "\n")
  k += 1
f3.write("\n")
f3.write("average perplexity of test dataset: "+str(avg_test))

import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
  
# creating the dataset
data1 = {'Neural Model average perplexity':22.275866192506143, 'Wittenbell Model average perplexity ':2.9793551949239}
data2 = {'Neural Model average perplexity':22.275866192506143, 'KNeserNey Model average perplexity ':5.045214858752456}
data3 = {'Neural Model average train perplexity':22.072964175198386, 'Neural Model average test perplexity':22.275866192506143}
d1 = list(data1.keys())
d2 = list(data1.values())
d3 = list(data2.keys())
d4 = list(data2.values())
d5 = list(data3.keys())
d6 = list(data3.values())
with PdfPages("Visualisation.pdf") as export_pdf:  
    fig = plt.figure(figsize = (10, 5))

    # creating the bar plot
    plt.bar(d1, d2, width = 0.4)
    plt.xlabel("comparision of different models")
    plt.ylabel("Average perplexity")
    plt.title("Model with their respective perplexity")
    export_pdf.savefig()
    plt.close()


    fig = plt.figure(figsize = (10, 5))
    print("\n\n\n\n\n")
    # creating the bar plot
    plt.bar(d3, d4, color ='maroon',
            width = 0.4)

    plt.xlabel("comparision of different models")
    plt.ylabel("Average perplexity")
    plt.title("Model with their respective perplexity")
    export_pdf.savefig()
    plt.close()


    fig = plt.figure(figsize = (10, 5))
    print("\n\n\n\n\n")
    # creating the bar plot
    plt.bar(d5, d6, color ='darkcyan',
            width = 0.4)

    plt.xlabel("comparision of different models")
    plt.ylabel("Average perplexity")
    plt.title("Model with their respective perplexity")
    export_pdf.savefig()
    plt.close()